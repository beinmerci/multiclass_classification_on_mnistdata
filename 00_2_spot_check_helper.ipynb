{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Problem type: Spot-check(shotgun) for Multiclass classification ML algorithm </h2>\n",
    "\n",
    "Spot-checking algorithms is about getting a quick assessment of a bunch of different algorithms on your machine learning problem so that you know what algorithms to focus on and what to discard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This python book intend to compare multiple algorithms based on accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression,LogisticRegressionCV,RidgeClassifier,RidgeClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier,ExtraTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple algorithms for multiclass classification problem.\n",
    "# Return : Dictionary with algorithm names and values\n",
    "def define_classifiers(classifiers=dict()):\n",
    "    \n",
    "    # linear models\n",
    "    classifiers['log_clf'] = LogisticRegression()\n",
    "    \n",
    "    alpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    for a in alpha:\n",
    "        classifiers['ridge_clf-'+str(a)] = RidgeClassifier(alpha=a)\n",
    "\n",
    "\n",
    "    classifiers['lsvc_clf'] = LinearSVC()  \n",
    "    \n",
    "    # non-linear models\n",
    "    n_neighbors = list(range(1, 31, 2))\n",
    "    for k in n_neighbors:\n",
    "        classifiers['knn_clf-'+str(k)] = KNeighborsClassifier(n_neighbors=k)\n",
    "        \n",
    "    classifiers['dt_clf'] = DecisionTreeClassifier()\n",
    "    classifiers['et_clf'] = ExtraTreeClassifier() \n",
    "        \n",
    "    classifiers['gnb_clf'] = GaussianNB()\n",
    "    classifiers['mlp_clf'] = MLPClassifier(alpha=1, max_iter=1000)\n",
    "    \n",
    "    # ensemble models\n",
    "    n_trees = 100\n",
    "    classifiers['rf_clf'] = RandomForestClassifier(n_estimators=n_trees)\n",
    "    classifiers['et_clf'] = ExtraTreesClassifier(n_estimators=n_trees)\n",
    "    \n",
    "    print('Defined %d classifiers' % len(classifiers))\n",
    "    return classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automate different steps in machine learning\n",
    "# Return tuple with Step name and value/funtion\n",
    "def create_pipeline(model):\n",
    "    steps = list()\n",
    "    # standardization\n",
    "    steps.append(('standardize', StandardScaler()))\n",
    "    # normalization\n",
    "    steps.append(('normalize', MinMaxScaler()))\n",
    "    # the model\n",
    "    steps.append(('classifier',model))\n",
    "    # create pipeline\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a single model\n",
    "# Returns : score evaluate a score by cross-validation\n",
    "def evaluate_model(train_features,train_label,test_features,test_label, model):\n",
    "    # create the pipeline\n",
    "    pipeline = create_pipeline(model)\n",
    "    # evaluate model\n",
    "    pipeline.fit(train_features,train_label)\n",
    "    scores= pipeline.score(test_features,test_label) #is calculating the difference between test_label and test_label_predictated (an accuracy measure), but you did not need to explicitly calculate test_label_predictated. The library does this internally.  \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a model and try to trap errors and and hide warnings\n",
    "# Returns : score evaluate a score by cross-validation\n",
    "def handle_warning(train_features,train_label,test_features,test_label, model):\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            scores = evaluate_model(train_features,train_label,test_features,test_label, model)\n",
    "    except:\n",
    "        scores = None\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different metric for multiclass classification\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#from-binary-to-multiclass-and-multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a dict of models {name:object}, returns {name:score}\n",
    "# Returns : {name:score}\n",
    "def evaluate_models(train_features,train_label,test_features,test_label, classifiers):\n",
    "    results = dict()\n",
    "    for name, model in classifiers.items():\n",
    "    # evaluate the model\n",
    "        scores = handle_warning(train_features,train_label,test_features,test_label, model)\n",
    "        # show process\n",
    "        if scores is not None:\n",
    "            # store a result\n",
    "            results[name] = scores\n",
    "            print('>%s: %.3f' % (name, scores))\n",
    "        else:\n",
    "            print('>%s: error' % name)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print and plot the top n results\n",
    "def summarize_results(results, maximize=True, top_n=10):\n",
    "    # check for no results\n",
    "    if len(results) == 0:\n",
    "        print('no results')\n",
    "        return\n",
    "    # determine how many results to summarize\n",
    "    n = min(top_n, len(results))\n",
    "    # create a list of (name,scores) tuples\n",
    "    scores = [(k,v) for k,v in results.items()]\n",
    "    # sort tuples by  score\n",
    "    scores = sorted(scores, key=lambda x: x[1])\n",
    "    # reverse for descending order (e.g. for accuracy)\n",
    "    if maximize:\n",
    "        scores = list(reversed(scores))\n",
    "    # retrieve the top n for summarization\n",
    "    names = [x[0] for x in scores[:n]]\n",
    "    scores = [results[x[0]] for x in scores[:n]]\n",
    "    # print the top n\n",
    "    print()\n",
    "    for i in range(n):\n",
    "        name = names[i]\n",
    "        score = results[name]\n",
    "        print('Rank=%d, Name=%s, Score=%.3f' % (i+1, name, score))\n",
    "    # boxplot for the top n\n",
    "    plt.plot(names, scores)\n",
    "    plt.title('Compare ML Algorithms')\n",
    "    _, labels = plt.xticks()\n",
    "    plt.setp(labels, rotation=90)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.savefig('modelcomparison.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
